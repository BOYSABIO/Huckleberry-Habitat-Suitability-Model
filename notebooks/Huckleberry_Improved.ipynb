{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping NaN: (6477, 12)\n",
      "Shape after dropping NaN: (6240, 12)\n",
      "Positive samples: 1646\n",
      "Negative samples: 4594\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/enriched/HB_PSEUDO_clean_elevation_soil.csv')\n",
    "\n",
    "# Remove location and time-specific columns (following your approach)\n",
    "df = df.drop(columns=['gbifID', 'gridmet_lat', 'gridmet_lon', 'gridmet_date', 'decimalLatitude', 'decimalLongitude'])\n",
    "\n",
    "# Convert datetime to numeric by extracting components with error handling\n",
    "def safe_datetime_parse(datetime_series):\n",
    "    \"\"\"Safely parse datetime series with mixed formats\"\"\"\n",
    "    parsed_dates = []\n",
    "    \n",
    "    for date_str in datetime_series:\n",
    "        try:\n",
    "            # Try parsing as full datetime first\n",
    "            parsed = pd.to_datetime(date_str)\n",
    "            parsed_dates.append(parsed)\n",
    "        except:\n",
    "            try:\n",
    "                # If that fails, try parsing as date only\n",
    "                parsed = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                parsed_dates.append(parsed)\n",
    "            except:\n",
    "                # If all else fails, use NaT (Not a Time)\n",
    "                parsed_dates.append(pd.NaT)\n",
    "    \n",
    "    return pd.Series(parsed_dates, index=datetime_series.index)\n",
    "\n",
    "# Parse the datetime column safely\n",
    "df['parsed_datetime'] = safe_datetime_parse(df['datetime'])\n",
    "\n",
    "# Extract only season (not specific month/day to avoid overfitting)\n",
    "df['month'] = df['parsed_datetime'].dt.month \n",
    "\n",
    "def month_to_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "# Apply the function to create a new 'season' column\n",
    "df['season'] = df['month'].apply(month_to_season)\n",
    "\n",
    "# Encode season as numbers\n",
    "season_map = {'Winter': 0, 'Spring': 1, 'Summer': 2, 'Fall': 3}\n",
    "df['season_num'] = df['season'].map(season_map)\n",
    "\n",
    "# Drop datetime columns and keep only season_num\n",
    "df = df.drop(['datetime', 'parsed_datetime', 'season', 'month'], axis=1)\n",
    "\n",
    "# Check for any remaining NaN values and drop them\n",
    "print(f\"Shape before dropping NaN: {df.shape}\")\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Shape after dropping NaN: {df.shape}\")\n",
    "print(f\"Positive samples: {df['occurrence'].sum()}\")\n",
    "print(f\"Negative samples: {(df['occurrence'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed\n"
     ]
    }
   ],
   "source": [
    "# Add only a few key interaction features that make ecological sense\n",
    "df['elevation_temp_interaction'] = df['elevation'] * df['air_temperature']\n",
    "df['temp_precip_interaction'] = df['air_temperature'] * df['precipitation_amount']\n",
    "\n",
    "# Simple suitability score based on elevation (huckleberries prefer mid-elevations)\n",
    "df['elevation_suitability'] = np.where(\n",
    "    (df['elevation'] >= 1000) & (df['elevation'] <= 2500), 1.0,\n",
    "    np.where((df['elevation'] >= 500) & (df['elevation'] <= 3000), 0.7, 0.3)\n",
    ")\n",
    "\n",
    "print(\"Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (4992, 14)\n",
      "Test set size: (1248, 14)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('occurrence', axis=1)\n",
    "y = df['occurrence']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create and train the model (following your approach)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calibrate the model to get better probability estimates\n",
    "rf_calibrated = CalibratedClassifierCV(rf, cv=5, method='isotonic')\n",
    "rf_calibrated.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       919\n",
      "           1       0.87      0.91      0.89       329\n",
      "\n",
      "    accuracy                           0.94      1248\n",
      "   macro avg       0.92      0.93      0.92      1248\n",
      "weighted avg       0.94      0.94      0.94      1248\n",
      "\n",
      "\n",
      "ROC AUC: 0.972\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                                      feature  importance\n",
      "9                                     soil_ph    0.284459\n",
      "6   surface_downwelling_shortwave_flux_in_air    0.103822\n",
      "11                 elevation_temp_interaction    0.099110\n",
      "2                           specific_humidity    0.096269\n",
      "8                                   elevation    0.095275\n",
      "5                potential_evapotranspiration    0.065585\n",
      "10                                 season_num    0.061067\n",
      "13                      elevation_suitability    0.047876\n",
      "4                 mean_vapor_pressure_deficit    0.040919\n",
      "0                             air_temperature    0.033104\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_calibrated.predict(X_test)\n",
    "y_proba = rf_calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print results\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nROC AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected columns: ['air_temperature', 'precipitation_amount', 'specific_humidity', 'relative_humidity', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'surface_downwelling_shortwave_flux_in_air', 'wind_speed', 'elevation', 'soil_ph', 'season_num', 'elevation_temp_interaction', 'temp_precip_interaction', 'elevation_suitability']\n",
      "Test data columns: ['elevation', 'air_temperature', 'precipitation_amount', 'soil_ph', 'specific_humidity', 'relative_humidity', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'surface_downwelling_shortwave_flux_in_air', 'wind_speed', 'season_num', 'elevation_temp_interaction', 'temp_precip_interaction', 'elevation_suitability']\n",
      "Final test data shape: (1000, 14)\n",
      "Final test data columns: ['air_temperature', 'precipitation_amount', 'specific_humidity', 'relative_humidity', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'surface_downwelling_shortwave_flux_in_air', 'wind_speed', 'elevation', 'soil_ph', 'season_num', 'elevation_temp_interaction', 'temp_precip_interaction', 'elevation_suitability']\n",
      "Synthetic test results:\n",
      "Mean probability: 0.235\n",
      "Max probability: 0.715\n",
      "Min probability: 0.006\n",
      "High probability samples (>0.6): 33\n",
      "Medium probability samples (0.3-0.6): 321\n",
      "Low probability samples (<0.3): 646\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic test coordinates\n",
    "np.random.seed(42)\n",
    "test_coords = []\n",
    "for _ in range(1000):\n",
    "    elevation = np.random.uniform(500, 3000)\n",
    "    air_temperature = np.random.uniform(280, 310)  # Kelvin range\n",
    "    precipitation_amount = np.random.uniform(30, 300)\n",
    "    soil_ph = np.random.uniform(5.5, 7.5)\n",
    "    specific_humidity = np.random.uniform(0.001, 0.015)\n",
    "    relative_humidity = np.random.uniform(20, 100)\n",
    "    mean_vapor_pressure_deficit = np.random.uniform(0.1, 2.5)\n",
    "    potential_evapotranspiration = np.random.uniform(0.5, 10)\n",
    "    surface_downwelling_shortwave_flux_in_air = np.random.uniform(50, 350)\n",
    "    wind_speed = np.random.uniform(1, 10)\n",
    "    season_num = np.random.randint(0, 4)\n",
    "    \n",
    "    test_coords.append({\n",
    "        'elevation': elevation,\n",
    "        'air_temperature': air_temperature,\n",
    "        'precipitation_amount': precipitation_amount,\n",
    "        'soil_ph': soil_ph,\n",
    "        'specific_humidity': specific_humidity,\n",
    "        'relative_humidity': relative_humidity,\n",
    "        'mean_vapor_pressure_deficit': mean_vapor_pressure_deficit,\n",
    "        'potential_evapotranspiration': potential_evapotranspiration,\n",
    "        'surface_downwelling_shortwave_flux_in_air': surface_downwelling_shortwave_flux_in_air,\n",
    "        'wind_speed': wind_speed,\n",
    "        'season_num': season_num\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(test_coords)\n",
    "\n",
    "# Add the engineered features\n",
    "test_df['elevation_temp_interaction'] = test_df['elevation'] * test_df['air_temperature']\n",
    "test_df['temp_precip_interaction'] = test_df['air_temperature'] * test_df['precipitation_amount']\n",
    "test_df['elevation_suitability'] = np.where(\n",
    "    (test_df['elevation'] >= 1000) & (test_df['elevation'] <= 2500), 1.0,\n",
    "    np.where((test_df['elevation'] >= 500) & (test_df['elevation'] <= 3000), 0.7, 0.3)\n",
    ")\n",
    "\n",
    "# Fix the feature mismatch issue\n",
    "# Get the exact column order from the training data\n",
    "expected_columns = X.columns.tolist()\n",
    "print(f\"Expected columns: {expected_columns}\")\n",
    "print(f\"Test data columns: {list(test_df.columns)}\")\n",
    "\n",
    "# Check for missing columns\n",
    "missing_columns = set(expected_columns) - set(test_df.columns)\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "    # Add missing columns with default values\n",
    "    for col in missing_columns:\n",
    "        test_df[col] = 0.0\n",
    "\n",
    "# Reorder test data columns to match training data exactly\n",
    "test_df = test_df[expected_columns]\n",
    "\n",
    "print(f\"Final test data shape: {test_df.shape}\")\n",
    "print(f\"Final test data columns: {list(test_df.columns)}\")\n",
    "\n",
    "# Make predictions\n",
    "synthetic_proba = rf_calibrated.predict_proba(test_df)[:, 1]\n",
    "\n",
    "print(f\"Synthetic test results:\")\n",
    "print(f\"Mean probability: {synthetic_proba.mean():.3f}\")\n",
    "print(f\"Max probability: {synthetic_proba.max():.3f}\")\n",
    "print(f\"Min probability: {synthetic_proba.min():.3f}\")\n",
    "print(f\"High probability samples (>0.6): {(synthetic_proba > 0.6).sum()}\")\n",
    "print(f\"Medium probability samples (0.3-0.6): {((synthetic_proba >= 0.3) & (synthetic_proba <= 0.6)).sum()}\")\n",
    "print(f\"Low probability samples (<0.3): {(synthetic_proba < 0.3).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the calibrated model and feature columns\n",
    "model_data = {\n",
    "    'rf_calibrated': rf_calibrated,\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'feature_importance': feature_importance\n",
    "}\n",
    "\n",
    "joblib.dump(model_data, '../models/improved_huckleberry_model.pkl')\n",
    "\n",
    "print(\"Improved model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 17 coordinates...\n",
      "Using latest available date: 2020-12-31\n",
      "Extracting air_temperature...\n",
      "Extracting precipitation_amount...\n",
      "Extracting specific_humidity...\n",
      "Extracting relative_humidity...\n",
      "Extracting mean_vapor_pressure_deficit...\n",
      "Extracting potential_evapotranspiration...\n",
      "Extracting surface_downwelling_shortwave_flux_in_air...\n",
      "Extracting wind_speed...\n",
      "✅ GridMET data extracted successfully!\n",
      "Getting elevation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting elevations: 100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting soil pH data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting soil pH:  88%|████████▊ | 15/17 [00:12<00:01,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Rate limited. Waiting 2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting soil pH: 100%|██████████| 17/17 [00:15<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environmental data extraction complete!\n",
      "Final test data shape: (17, 14)\n",
      "Features: ['air_temperature', 'precipitation_amount', 'specific_humidity', 'relative_humidity', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'surface_downwelling_shortwave_flux_in_air', 'wind_speed', 'elevation', 'soil_ph', 'season_num', 'elevation_temp_interaction', 'temp_precip_interaction', 'elevation_suitability']\n",
      "\n",
      "Prediction Results:\n",
      "============================================================\n",
      "(44.500000, -116.500000): NO HUCKLEBERRY (Probability: 0.011)\n",
      "(44.600000, -116.400000): NO HUCKLEBERRY (Probability: 0.011)\n",
      "(44.700000, -116.300000): NO HUCKLEBERRY (Probability: 0.014)\n",
      "(44.499669, -111.329396): NO HUCKLEBERRY (Probability: 0.006)\n",
      "(48.687832, -116.924988): NO HUCKLEBERRY (Probability: 0.029)\n",
      "(48.698033, -116.953418): NO HUCKLEBERRY (Probability: 0.029)\n",
      "(48.721255, -116.973486): NO HUCKLEBERRY (Probability: 0.031)\n",
      "(48.769863, -116.948780): NO HUCKLEBERRY (Probability: 0.055)\n",
      "(48.802889, -116.983530): NO HUCKLEBERRY (Probability: 0.022)\n",
      "(48.818715, -116.936533): NO HUCKLEBERRY (Probability: 0.010)\n",
      "(48.790450, -117.027455): NO HUCKLEBERRY (Probability: 0.055)\n",
      "(48.815164, -116.981784): NO HUCKLEBERRY (Probability: 0.022)\n",
      "(48.789158, -116.957771): NO HUCKLEBERRY (Probability: 0.055)\n",
      "(48.796396, -116.970464): NO HUCKLEBERRY (Probability: 0.022)\n",
      "(48.785087, -116.933929): NO HUCKLEBERRY (Probability: 0.055)\n",
      "(48.805554, -116.888647): NO HUCKLEBERRY (Probability: 0.010)\n",
      "(48.751289, -116.994538): NO HUCKLEBERRY (Probability: 0.031)\n",
      "\n",
      "Summary:\n",
      "High probability (>0.6): 0\n",
      "Medium probability (0.3-0.6): 0\n",
      "Low probability (<0.3): 17\n",
      "Mean probability: 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Your coordinates (lat, lon pairs)\n",
    "coordinates = [\n",
    "    (44.5, -116.5), (44.6, -116.4), (44.7, -116.3), (44.499669, -111.329396),\n",
    "    (48.687832, -116.924988), (48.698033, -116.953418), (48.721255, -116.973486),\n",
    "    (48.769863, -116.948780), (48.802889, -116.983530), (48.818715, -116.936533),\n",
    "    (48.790450, -117.027455), (48.815164, -116.981784), (48.789158, -116.957771),\n",
    "    (48.796396, -116.970464), (48.785087, -116.933929), (48.805554, -116.888647),\n",
    "    (48.751289, -116.994538)\n",
    "]\n",
    "\n",
    "# Create DataFrame with coordinates\n",
    "coords_df = pd.DataFrame(coordinates, columns=['decimalLatitude', 'decimalLongitude'])\n",
    "\n",
    "print(f\"Testing {len(coords_df)} coordinates...\")\n",
    "\n",
    "# Connect to Planetary Computer and load GridMET (following your pattern)\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "asset = catalog.get_collection(\"gridmet\").assets[\"zarr-abfs\"]\n",
    "ds = xr.open_zarr(\n",
    "    asset.href,\n",
    "    storage_options=asset.extra_fields[\"xarray:storage_options\"],\n",
    "    **asset.extra_fields[\"xarray:open_kwargs\"],\n",
    "    chunks=\"auto\"\n",
    ")\n",
    "\n",
    "# Get the most recent date available\n",
    "latest_time = ds.time.values.max()\n",
    "print(f\"Using latest available date: {pd.to_datetime(latest_time).date()}\")\n",
    "\n",
    "# Variables to extract (matching your pattern)\n",
    "VARS_TO_EXTRACT = [\n",
    "    \"air_temperature\",\n",
    "    \"precipitation_amount\", \n",
    "    \"specific_humidity\",\n",
    "    \"relative_humidity\",\n",
    "    \"mean_vapor_pressure_deficit\",\n",
    "    \"potential_evapotranspiration\",\n",
    "    \"surface_downwelling_shortwave_flux_in_air\",\n",
    "    \"wind_speed\"\n",
    "]\n",
    "\n",
    "# Snap coordinates to nearest GridMET grid (following your pattern)\n",
    "def snap_to_grid(val, grid):\n",
    "    return grid[np.abs(grid - val).argmin()]\n",
    "\n",
    "lats = coords_df['decimalLatitude'].values\n",
    "lons = coords_df['decimalLongitude'].values\n",
    "\n",
    "snapped_lats = np.array([snap_to_grid(lat, ds.lat.values) for lat in lats])\n",
    "snapped_lons = np.array([snap_to_grid(lon, ds.lon.values) for lon in lons])\n",
    "\n",
    "# Build DataFrame\n",
    "test_df = pd.DataFrame({\n",
    "    \"decimalLatitude\": snapped_lats,\n",
    "    \"decimalLongitude\": snapped_lons\n",
    "})\n",
    "\n",
    "# Extract all variables at once for the latest date\n",
    "for var in VARS_TO_EXTRACT:\n",
    "    print(f\"Extracting {var}...\")\n",
    "    values = ds[var].sel(\n",
    "        time=latest_time,\n",
    "        lat=xr.DataArray(snapped_lats, dims=\"points\"),\n",
    "        lon=xr.DataArray(snapped_lons, dims=\"points\")\n",
    "    ).values\n",
    "    test_df[var] = values\n",
    "\n",
    "# Add the gridmet_date column\n",
    "test_df['gridmet_date'] = pd.to_datetime(latest_time)\n",
    "\n",
    "print(\"✅ GridMET data extracted successfully!\")\n",
    "\n",
    "# Now get elevation data (following your elevation.py pattern)\n",
    "def get_elevation(lat, lon):\n",
    "    \"\"\"Query Open-Elevation API for elevation.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"https://api.open-elevation.com/api/v1/lookup\", \n",
    "                              params={\"locations\": f\"{lat},{lon}\"})\n",
    "        if response.status_code == 200:\n",
    "            elevation = response.json()[\"results\"][0][\"elevation\"]\n",
    "            return elevation\n",
    "        else:\n",
    "            print(f\"⚠️ API error: {response.status_code}\")\n",
    "            return 1000  # Default elevation\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to get elevation for {lat}, {lon}: {e}\")\n",
    "        return 1000  # Default elevation\n",
    "\n",
    "print(\"Getting elevation data...\")\n",
    "elevations = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Getting elevations\"):\n",
    "    lat, lon = row['decimalLatitude'], row['decimalLongitude']\n",
    "    elevation = get_elevation(lat, lon)\n",
    "    elevations.append(elevation)\n",
    "    time.sleep(0.5)  # Be polite to API\n",
    "\n",
    "test_df['elevation'] = elevations\n",
    "\n",
    "# Get soil pH data (following your soil_data.py pattern)\n",
    "def get_soil_ph(lat, lon, retries=3, base_delay=2):\n",
    "    \"\"\"Fetch soil pH with retries.\"\"\"\n",
    "    url = f\"https://rest.isric.org/soilgrids/v2.0/properties/query?lat={lat}&lon={lon}&property=phh2o&depth=15-30cm&value=mean\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 429:\n",
    "                wait = base_delay * (2 ** attempt)\n",
    "                print(f\"⏳ Rate limited. Waiting {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            layers = data.get(\"properties\", {}).get(\"layers\", [])\n",
    "            for layer in layers:\n",
    "                if layer[\"name\"] == \"phh2o\":\n",
    "                    values = layer[\"depths\"][0][\"values\"]\n",
    "                    raw_ph = values.get(\"mean\")\n",
    "                    if raw_ph is not None:\n",
    "                        return round(raw_ph / 10.0, 2)  # Convert to pH scale\n",
    "            return 6.5  # Default pH\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error at ({lat}, {lon}): {e}\")\n",
    "            time.sleep(base_delay * (2 ** attempt))\n",
    "    \n",
    "    return 6.5  # Default pH\n",
    "\n",
    "print(\"Getting soil pH data...\")\n",
    "soil_ph_values = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Getting soil pH\"):\n",
    "    lat, lon = row['decimalLatitude'], row['decimalLongitude']\n",
    "    ph = get_soil_ph(lat, lon)\n",
    "    soil_ph_values.append(ph)\n",
    "    time.sleep(0.5)  # Be polite to API\n",
    "\n",
    "test_df['soil_ph'] = soil_ph_values\n",
    "\n",
    "print(\"✅ Environmental data extraction complete!\")\n",
    "\n",
    "# Now apply the same transformations as your training data\n",
    "# Parse datetime and create season features\n",
    "test_df['parsed_datetime'] = pd.to_datetime(test_df['gridmet_date'])\n",
    "test_df['month'] = test_df['parsed_datetime'].dt.month\n",
    "\n",
    "def month_to_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "test_df['season'] = test_df['month'].apply(month_to_season)\n",
    "season_map = {'Winter': 0, 'Spring': 1, 'Summer': 2, 'Fall': 3}\n",
    "test_df['season_num'] = test_df['season'].map(season_map)\n",
    "\n",
    "# Add engineered features\n",
    "test_df['elevation_temp_interaction'] = test_df['elevation'] * test_df['air_temperature']\n",
    "test_df['temp_precip_interaction'] = test_df['air_temperature'] * test_df['precipitation_amount']\n",
    "test_df['elevation_suitability'] = np.where(\n",
    "    (test_df['elevation'] >= 1000) & (test_df['elevation'] <= 2500), 1.0,\n",
    "    np.where((test_df['elevation'] >= 500) & (test_df['elevation'] <= 3000), 0.7, 0.3)\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns and reorder to match training data\n",
    "test_df = test_df.drop(['parsed_datetime', 'season', 'month', 'gridmet_date'], axis=1)\n",
    "\n",
    "# Ensure we have the same columns as training data\n",
    "expected_columns = X.columns.tolist()\n",
    "missing_columns = set(expected_columns) - set(test_df.columns)\n",
    "if missing_columns:\n",
    "    print(f\"Adding missing columns: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        test_df[col] = 0.0\n",
    "\n",
    "# Reorder columns to match training data\n",
    "test_df = test_df[expected_columns]\n",
    "\n",
    "print(f\"Final test data shape: {test_df.shape}\")\n",
    "print(f\"Features: {list(test_df.columns)}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_calibrated.predict(test_df)\n",
    "probabilities = rf_calibrated.predict_proba(test_df)[:, 1]\n",
    "\n",
    "# Add predictions back to original coordinates\n",
    "results_df = coords_df.copy()\n",
    "results_df['prediction'] = predictions\n",
    "results_df['probability'] = probabilities\n",
    "\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in results_df.iterrows():\n",
    "    lat, lon = row['decimalLatitude'], row['decimalLongitude']\n",
    "    pred = \"HUCKLEBERRY\" if row['prediction'] == 1 else \"NO HUCKLEBERRY\"\n",
    "    prob = row['probability']\n",
    "    print(f\"({lat:.6f}, {lon:.6f}): {pred} (Probability: {prob:.3f})\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"High probability (>0.6): {(probabilities > 0.6).sum()}\")\n",
    "print(f\"Medium probability (0.3-0.6): {((probabilities >= 0.3) & (probabilities <= 0.6)).sum()}\")\n",
    "print(f\"Low probability (<0.3): {(probabilities < 0.3).sum()}\")\n",
    "print(f\"Mean probability: {probabilities.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING WITH SUMMER 2019 CONDITIONS ===\n",
      "Using summer date: 2019-07-15\n",
      "Extracting air_temperature for summer 2019...\n",
      "Extracting precipitation_amount for summer 2019...\n",
      "Extracting specific_humidity for summer 2019...\n",
      "Extracting relative_humidity for summer 2019...\n",
      "Extracting mean_vapor_pressure_deficit for summer 2019...\n",
      "Extracting potential_evapotranspiration for summer 2019...\n",
      "Extracting surface_downwelling_shortwave_flux_in_air for summer 2019...\n",
      "Extracting wind_speed for summer 2019...\n",
      "\n",
      "=== SUMMER 2019 PREDICTION RESULTS ===\n",
      "============================================================\n",
      "(44.500000, -116.500000): NO HUCKLEBERRY (Probability: 0.258)\n",
      "(44.600000, -116.400000): NO HUCKLEBERRY (Probability: 0.378)\n",
      "(44.700000, -116.300000): HUCKLEBERRY (Probability: 0.963)\n",
      "(44.499669, -111.329396): HUCKLEBERRY (Probability: 0.920)\n",
      "(48.687832, -116.924988): HUCKLEBERRY (Probability: 0.961)\n",
      "(48.698033, -116.953418): HUCKLEBERRY (Probability: 0.961)\n",
      "(48.721255, -116.973486): HUCKLEBERRY (Probability: 0.916)\n",
      "(48.769863, -116.948780): HUCKLEBERRY (Probability: 0.969)\n",
      "(48.802889, -116.983530): HUCKLEBERRY (Probability: 0.782)\n",
      "(48.818715, -116.936533): HUCKLEBERRY (Probability: 0.632)\n",
      "(48.790450, -117.027455): HUCKLEBERRY (Probability: 0.935)\n",
      "(48.815164, -116.981784): HUCKLEBERRY (Probability: 0.782)\n",
      "(48.789158, -116.957771): HUCKLEBERRY (Probability: 0.938)\n",
      "(48.796396, -116.970464): HUCKLEBERRY (Probability: 0.782)\n",
      "(48.785087, -116.933929): HUCKLEBERRY (Probability: 0.969)\n",
      "(48.805554, -116.888647): HUCKLEBERRY (Probability: 0.756)\n",
      "(48.751289, -116.994538): HUCKLEBERRY (Probability: 0.916)\n",
      "\n",
      "Summer 2019 Summary:\n",
      "High probability (>0.6): 15\n",
      "Medium probability (0.3-0.6): 1\n",
      "Low probability (<0.3): 1\n",
      "Mean probability: 0.813\n",
      "\n",
      "=== COMPARISON ===\n",
      "Winter mean probability: 0.028\n",
      "Summer 2019 mean probability: 0.813\n",
      "Improvement: 2844.8%\n",
      "\n",
      "=== SUMMER 2019 DATA SAMPLE ===\n",
      "Temperature range: 277.4K to 287.7K\n",
      "Solar radiation range: 221.7 to 321.5 W/m²\n",
      "Season: 2 (Summer)\n",
      "Precipitation range: 0.0 to 2.2 mm\n"
     ]
    }
   ],
   "source": [
    "# Test with summer 2019 conditions\n",
    "print(\"=== TESTING WITH SUMMER 2019 CONDITIONS ===\")\n",
    "\n",
    "# Use summer 2019 date\n",
    "summer_time = pd.Timestamp('2019-07-15')  # Mid-summer 2019\n",
    "print(f\"Using summer date: {summer_time.date()}\")\n",
    "\n",
    "# Re-extract GridMET data for summer 2019\n",
    "summer_test_df = pd.DataFrame({\n",
    "    \"decimalLatitude\": snapped_lats,\n",
    "    \"decimalLongitude\": snapped_lons\n",
    "})\n",
    "\n",
    "# Extract summer data\n",
    "for var in VARS_TO_EXTRACT:\n",
    "    print(f\"Extracting {var} for summer 2019...\")\n",
    "    values = ds[var].sel(\n",
    "        time=summer_time,\n",
    "        lat=xr.DataArray(snapped_lats, dims=\"points\"),\n",
    "        lon=xr.DataArray(snapped_lons, dims=\"points\")\n",
    "    ).values\n",
    "    summer_test_df[var] = values\n",
    "\n",
    "# Add summer date\n",
    "summer_test_df['gridmet_date'] = summer_time\n",
    "\n",
    "# Reuse elevation and soil data from before\n",
    "summer_test_df['elevation'] = test_df['elevation'].values\n",
    "summer_test_df['soil_ph'] = test_df['soil_ph'].values\n",
    "\n",
    "# Apply transformations for summer\n",
    "summer_test_df['parsed_datetime'] = pd.to_datetime(summer_test_df['gridmet_date'])\n",
    "summer_test_df['month'] = summer_test_df['parsed_datetime'].dt.month\n",
    "summer_test_df['season'] = summer_test_df['month'].apply(month_to_season)\n",
    "summer_test_df['season_num'] = summer_test_df['season'].map(season_map)\n",
    "\n",
    "# Add engineered features\n",
    "summer_test_df['elevation_temp_interaction'] = summer_test_df['elevation'] * summer_test_df['air_temperature']\n",
    "summer_test_df['temp_precip_interaction'] = summer_test_df['air_temperature'] * summer_test_df['precipitation_amount']\n",
    "summer_test_df['elevation_suitability'] = np.where(\n",
    "    (summer_test_df['elevation'] >= 1000) & (summer_test_df['elevation'] <= 2500), 1.0,\n",
    "    np.where((summer_test_df['elevation'] >= 500) & (summer_test_df['elevation'] <= 3000), 0.7, 0.3)\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "summer_test_df = summer_test_df.drop(['parsed_datetime', 'season', 'month', 'gridmet_date'], axis=1)\n",
    "\n",
    "# Ensure same columns as training data\n",
    "missing_columns = set(expected_columns) - set(summer_test_df.columns)\n",
    "if missing_columns:\n",
    "    for col in missing_columns:\n",
    "        summer_test_df[col] = 0.0\n",
    "\n",
    "summer_test_df = summer_test_df[expected_columns]\n",
    "\n",
    "# Make predictions\n",
    "summer_predictions = rf_calibrated.predict(summer_test_df)\n",
    "summer_probabilities = rf_calibrated.predict_proba(summer_test_df)[:, 1]\n",
    "\n",
    "# Show results\n",
    "print(\"\\n=== SUMMER 2019 PREDICTION RESULTS ===\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in coords_df.iterrows():\n",
    "    lat, lon = row['decimalLatitude'], row['decimalLongitude']\n",
    "    pred = \"HUCKLEBERRY\" if summer_predictions[idx] == 1 else \"NO HUCKLEBERRY\"\n",
    "    prob = summer_probabilities[idx]\n",
    "    print(f\"({lat:.6f}, {lon:.6f}): {pred} (Probability: {prob:.3f})\")\n",
    "\n",
    "print(f\"\\nSummer 2019 Summary:\")\n",
    "print(f\"High probability (>0.6): {(summer_probabilities > 0.6).sum()}\")\n",
    "print(f\"Medium probability (0.3-0.6): {((summer_probabilities >= 0.3) & (summer_probabilities <= 0.6)).sum()}\")\n",
    "print(f\"Low probability (<0.3): {(summer_probabilities < 0.3).sum()}\")\n",
    "print(f\"Mean probability: {summer_probabilities.mean():.3f}\")\n",
    "\n",
    "# Compare summer vs winter data\n",
    "print(f\"\\n=== COMPARISON ===\")\n",
    "print(f\"Winter mean probability: {probabilities.mean():.3f}\")\n",
    "print(f\"Summer 2019 mean probability: {summer_probabilities.mean():.3f}\")\n",
    "print(f\"Improvement: {((summer_probabilities.mean() - probabilities.mean()) / probabilities.mean() * 100):.1f}%\")\n",
    "\n",
    "# Show some sample summer data\n",
    "print(f\"\\n=== SUMMER 2019 DATA SAMPLE ===\")\n",
    "print(f\"Temperature range: {summer_test_df['air_temperature'].min():.1f}K to {summer_test_df['air_temperature'].max():.1f}K\")\n",
    "print(f\"Solar radiation range: {summer_test_df['surface_downwelling_shortwave_flux_in_air'].min():.1f} to {summer_test_df['surface_downwelling_shortwave_flux_in_air'].max():.1f} W/m²\")\n",
    "print(f\"Season: {summer_test_df['season_num'].iloc[0]} (Summer)\")\n",
    "print(f\"Precipitation range: {summer_test_df['precipitation_amount'].min():.1f} to {summer_test_df['precipitation_amount'].max():.1f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING ORIGINAL MODEL FEATURES ===\n",
      "Original model expects these features (in order):\n",
      "  0: air_temperature\n",
      "  1: precipitation_amount\n",
      "  2: specific_humidity\n",
      "  3: relative_humidity\n",
      "  4: mean_vapor_pressure_deficit\n",
      "  5: potential_evapotranspiration\n",
      "  6: surface_downwelling_shortwave_flux_in_air\n",
      "  7: wind_speed\n",
      "  8: elevation\n",
      "  9: soil_ph\n",
      "  10: year\n",
      "  11: month\n",
      "  12: day\n",
      "  13: season_num\n",
      "Total features: 14\n",
      "\n",
      "Features we're providing:\n",
      "  0: year\n",
      "  1: month\n",
      "  2: day\n",
      "  3: air_temperature\n",
      "  4: precipitation_amount\n",
      "  5: specific_humidity\n",
      "  6: relative_humidity\n",
      "  7: mean_vapor_pressure_deficit\n",
      "  8: potential_evapotranspiration\n",
      "  9: surface_downwelling_shortwave_flux_in_air\n",
      "  10: wind_speed\n",
      "  11: elevation\n",
      "  12: soil_ph\n",
      "  13: season_num\n",
      "Total features: 14\n",
      "\n",
      "=== FEATURE COMPARISON ===\n",
      "Missing from our data: set()\n",
      "Extra in our data: set()\n",
      "\n",
      "=== CREATING CORRECT FEATURE SET ===\n",
      "Correct feature set shape: (17, 14)\n",
      "Correct features: ['air_temperature', 'precipitation_amount', 'specific_humidity', 'relative_humidity', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'surface_downwelling_shortwave_flux_in_air', 'wind_speed', 'elevation', 'soil_ph', 'year', 'month', 'day', 'season_num']\n",
      "\n",
      "=== TESTING ORIGINAL MODEL WITH CORRECT FEATURES ===\n",
      "✅ Original model predictions successful!\n",
      "\n",
      "=== MODEL COMPARISON RESULTS ===\n",
      "================================================================================\n",
      "Location                  Improved Model       Original Model       Difference     \n",
      "                          Pred     Prob     Pred     Prob     Prob Diff      \n",
      "--------------------------------------------------------------------------------\n",
      "(44.500, -116.500)        NO       0.258    NO       0.340    -0.082  \n",
      "(44.600, -116.400)        NO       0.378    NO       0.370    +0.008  \n",
      "(44.700, -116.300)        HUCKLE   0.963    HUCKLE   0.950    +0.013  \n",
      "(44.500, -111.329)        HUCKLE   0.920    HUCKLE   0.940    -0.020  \n",
      "(48.688, -116.925)        HUCKLE   0.961    HUCKLE   0.960    +0.001  \n",
      "(48.698, -116.953)        HUCKLE   0.961    HUCKLE   0.960    +0.001  \n",
      "(48.721, -116.973)        HUCKLE   0.916    HUCKLE   0.930    -0.014  \n",
      "(48.770, -116.949)        HUCKLE   0.969    HUCKLE   0.970    -0.001  \n",
      "(48.803, -116.984)        HUCKLE   0.782    HUCKLE   0.750    +0.032  \n",
      "(48.819, -116.937)        HUCKLE   0.632    HUCKLE   0.620    +0.012  \n",
      "(48.790, -117.027)        HUCKLE   0.935    HUCKLE   0.960    -0.025  \n",
      "(48.815, -116.982)        HUCKLE   0.782    HUCKLE   0.750    +0.032  \n",
      "(48.789, -116.958)        HUCKLE   0.938    HUCKLE   0.950    -0.012  \n",
      "(48.796, -116.970)        HUCKLE   0.782    HUCKLE   0.750    +0.032  \n",
      "(48.785, -116.934)        HUCKLE   0.969    HUCKLE   0.970    -0.001  \n",
      "(48.806, -116.889)        HUCKLE   0.756    HUCKLE   0.740    +0.016  \n",
      "(48.751, -116.995)        HUCKLE   0.916    HUCKLE   0.930    -0.014  \n",
      "\n",
      "=== SUMMARY ===\n",
      "Improved model mean probability: 0.813\n",
      "Original model mean probability: 0.814\n",
      "Improvement: -0.1%\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what features the original model expects\n",
    "print(\"=== DEBUGGING ORIGINAL MODEL FEATURES ===\")\n",
    "\n",
    "# Check what features the original model was trained on\n",
    "if hasattr(original_model, 'feature_names_in_'):\n",
    "    original_features = list(original_model.feature_names_in_)\n",
    "    print(f\"Original model expects these features (in order):\")\n",
    "    for i, feature in enumerate(original_features):\n",
    "        print(f\"  {i}: {feature}\")\n",
    "    print(f\"Total features: {len(original_features)}\")\n",
    "else:\n",
    "    print(\"❌ Original model doesn't have feature_names_in_ attribute\")\n",
    "    # Try to get features from the saved model data\n",
    "    if isinstance(original_model_data, dict) and 'feature_names' in original_model_data:\n",
    "        original_features = original_model_data['feature_names']\n",
    "        print(f\"Original model features from saved data:\")\n",
    "        for i, feature in enumerate(original_features):\n",
    "            print(f\"  {i}: {feature}\")\n",
    "    else:\n",
    "        print(\"❌ Could not determine original model features\")\n",
    "        original_features = None\n",
    "\n",
    "# Check what features we're providing\n",
    "print(f\"\\nFeatures we're providing:\")\n",
    "for i, feature in enumerate(original_compatible_df.columns):\n",
    "    print(f\"  {i}: {feature}\")\n",
    "print(f\"Total features: {len(original_compatible_df.columns)}\")\n",
    "\n",
    "# Compare the two lists\n",
    "if original_features is not None:\n",
    "    print(f\"\\n=== FEATURE COMPARISON ===\")\n",
    "    print(f\"Missing from our data: {set(original_features) - set(original_compatible_df.columns)}\")\n",
    "    print(f\"Extra in our data: {set(original_compatible_df.columns) - set(original_features)}\")\n",
    "    \n",
    "    # Create the correct feature set\n",
    "    print(f\"\\n=== CREATING CORRECT FEATURE SET ===\")\n",
    "    correct_df = pd.DataFrame()\n",
    "    \n",
    "    for feature in original_features:\n",
    "        if feature in original_compatible_df.columns:\n",
    "            correct_df[feature] = original_compatible_df[feature]\n",
    "        else:\n",
    "            # Add missing features with default values\n",
    "            print(f\"Adding missing feature '{feature}' with default value\")\n",
    "            if feature in ['year', 'month', 'day']:\n",
    "                correct_df[feature] = 2019 if feature == 'year' else 7 if feature == 'month' else 15\n",
    "            elif feature == 'season_num':\n",
    "                correct_df[feature] = 2  # Summer\n",
    "            else:\n",
    "                correct_df[feature] = 0.0  # Default for other features\n",
    "    \n",
    "    print(f\"Correct feature set shape: {correct_df.shape}\")\n",
    "    print(f\"Correct features: {list(correct_df.columns)}\")\n",
    "    \n",
    "    # Now test the original model\n",
    "    print(f\"\\n=== TESTING ORIGINAL MODEL WITH CORRECT FEATURES ===\")\n",
    "    try:\n",
    "        original_predictions = original_model.predict(correct_df)\n",
    "        original_probabilities = original_model.predict_proba(correct_df)[:, 1]\n",
    "        print(\"✅ Original model predictions successful!\")\n",
    "        \n",
    "        # Now we can do the comparison\n",
    "        print(f\"\\n=== MODEL COMPARISON RESULTS ===\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Test improved model (with engineered features)\n",
    "        improved_predictions = rf_calibrated.predict(summer_test_df)\n",
    "        improved_probabilities = rf_calibrated.predict_proba(summer_test_df)[:, 1]\n",
    "        \n",
    "        # Display results side by side\n",
    "        print(f\"{'Location':<25} {'Improved Model':<20} {'Original Model':<20} {'Difference':<15}\")\n",
    "        print(f\"{'':<25} {'Pred':<8} {'Prob':<8} {'Pred':<8} {'Prob':<8} {'Prob Diff':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for idx, row in coords_df.iterrows():\n",
    "            lat, lon = row['decimalLatitude'], row['decimalLongitude']\n",
    "            location = f\"({lat:.3f}, {lon:.3f})\"\n",
    "            \n",
    "            # Improved model results\n",
    "            improved_pred = \"HUCKLE\" if improved_predictions[idx] == 1 else \"NO\"\n",
    "            improved_prob = improved_probabilities[idx]\n",
    "            \n",
    "            # Original model results\n",
    "            original_pred = \"HUCKLE\" if original_predictions[idx] == 1 else \"NO\"\n",
    "            original_prob = original_probabilities[idx]\n",
    "            \n",
    "            # Difference\n",
    "            prob_diff = improved_prob - original_prob\n",
    "            \n",
    "            print(f\"{location:<25} {improved_pred:<8} {improved_prob:<8.3f} {original_pred:<8} {original_prob:<8.3f} {prob_diff:<+8.3f}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n=== SUMMARY ===\")\n",
    "        print(f\"Improved model mean probability: {improved_probabilities.mean():.3f}\")\n",
    "        print(f\"Original model mean probability: {original_probabilities.mean():.3f}\")\n",
    "        print(f\"Improvement: {((improved_probabilities.mean() - original_probabilities.mean()) / original_probabilities.mean() * 100):.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Still getting error: {e}\")\n",
    "        print(\"Please check the original model file or provide more details about how it was trained\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot proceed without knowing the original model's expected features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone-Microsoft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
